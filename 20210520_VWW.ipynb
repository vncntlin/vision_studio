{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210520_VWW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTKNU1Iwyrts"
      },
      "source": [
        "# 下載資料\n",
        "\n",
        "我生好person, bicycle, car, traffic light的資料了，請至這個google drive連結下載然後傳到你自己的google drive\n",
        "https://drive.google.com/drive/folders/1ZOpWhs83X1dMMqB9NokK9sn2SDMorpqG?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qyiNj2tyriv"
      },
      "source": [
        "# 自己準備資料(可先跳過)\n",
        "\n",
        "如果你想自己弄別的class，或調整small object threshold，請把下面三個檔案換成這個zip檔裡面的檔案\n",
        "1. models/research/slim內的download_and_convert_data.py\n",
        "2. 和models/reseach/slim/datasets內的download_and_convert_visualwakewords.py和download_and_convert_visualwakewords_lib.py\n",
        "\n",
        "zip連結: https://drive.google.com/file/d/1acqxB0kmCENUxBABTjxSLjEETeNe9xe2/view?usp=sharing\n",
        "\n",
        "可以修改download_and_convert_data.py的small_object_area_threshold和foreground_class_of_interest\n",
        "\n",
        "當然也可以修改圖片resize的大小(不過model input也要改)\n",
        "\n",
        "然後在models/research/slim內執行\n",
        "python download_and_convert_data.py --dataset_name=visualwakewords --dataset_dir=/tmp/visualwakewords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311MPjKnBqRU"
      },
      "source": [
        "# 後續燒錄進板子(可先跳過)\n",
        "1. 下載tflite擋下來用xxd -i filename.tflite person_detect_model_data.cc把他轉成C++檔\n",
        "2. 包成C++檔之後，改掉檔案的頭尾。照抄原本專案裡的person_detect_model_data.cc頭尾就好了。\n",
        "3. 記得main_function.cc裡面的mutable_op_resolver下，要添加這個net需要的operations(OP必須照字母順序!)，model大時有可能需要改大kTensorArenaSize，不過太大的話也可能compile沒過。一些常見的error如下:\n",
        "\n",
        "  *   Arena size is too small for all buffers. Needed 176720 but only 137424 was available.\n",
        "AllocateTensors() failed\n",
        "  \n",
        "    -->改大kTensorArenaSize\n",
        "\n",
        "  *  Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'\n",
        "Failed to get registration from op code MAX_POOL_2D\n",
        "Failed starting model allocation.\n",
        "\n",
        "    -->改mutable_op_resolver的OP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9clHT8Wnhfl"
      },
      "source": [
        "# 訓練模型\n",
        "\n",
        "執行下一格之後照著指示做，可以讓colab存取google drive內的東西"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHh69wTBHwz8",
        "outputId": "a8d4a73a-781d-4124-be52-effb02f3193e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVZplqXQ3URV"
      },
      "source": [
        "!rm -rf dues_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFMi7MgynQVM"
      },
      "source": [
        "下文的/content/drive/My Drive/20210521_vww_data/dues_data_person.zip請修改成自己的file path\n",
        "\n",
        "目前採用的data:\n",
        "*   dues_data_person.zip\n",
        "*   dues_data_car_new.zip (有加額外資料)\n",
        "*   dues_data_bike_new.zip (包含bike和motorbike、額外資料、coco2017的資料，勉強可train起來)\n",
        "\n",
        "未採用，但仍可使用的data:\n",
        "*   dues_data_car.zip\n",
        "*   dues_data_bicycle.zip (train不起來)\n",
        "*   dues_data_bike.zip (包含bike和motorbike、額外資料，train不太起來)\n",
        "*   dues_data_bus.zip (包含bus和truck，train不太起來，而且通常car也會偵測到bus或truck)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16dQ5dN9musu"
      },
      "source": [
        "!unzip '/content/drive/My Drive/20210521_vww_data/dues_data_bike_new.zip' -d '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5imohVFtJ5Y"
      },
      "source": [
        "我寫了三種model: simple CNN, ResNet-like, MobileNetV2-like(full & small)\n",
        "\n",
        "2021/7/15 update: 新的mobileNet-like\n",
        "\n",
        "注意事項:\n",
        "1. model架構可以亂改，但注意model.summary()的時候output出來的total parameter數量最好不要超過500,000不然一定oversize。我暫時是試在100,000以內了啦\n",
        "2. ResNet(2 blocks)和MobileNetV2有點train不動，建議用CNN\n",
        "3. paper的準度是75%，有train到70%就還算不錯\n",
        "4. 因為資料量大，建議每次train之前kill一下OS，釋放出RAM(執行下方的block)\n",
        "5. 若每個class資料數量差異大，可以採用weighted classes。請參考: https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReiUUgQlwLOR"
      },
      "source": [
        "每次train之前，建議跑下面這個block以釋放RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmxurNX5vpwC"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ENkRgw-zYu"
      },
      "source": [
        "複製檔案到drive用的而已"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SgEEEsCqgyF",
        "outputId": "6b132d33-3cfd-4876-f438-d605dce595d7"
      },
      "source": [
        "!dir\n",
        "!cp vww_mobile_v5_25.tflite '/content/drive/My Drive/20210521_vww_data/vww_mobile_v5_25_bike_new.tflite'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check_point  drive  dues_data  sample_data  vww_mobile_v5_25.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN__ru2wLjo"
      },
      "source": [
        "### 下面幾個儲存格請選一個執行即可"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1crRNWU1AtPE"
      },
      "source": [
        "MobileNetV1-like(建議使用)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J713fUUPTNMU",
        "outputId": "b591eeda-06aa-4c44-a4ef-79364121f701"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1), alpha=1.0, classes=2):\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # ---build block---\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=2, use_bias=False, name='Conv1', activation='relu')(img_input)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=5)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=1, use_bias=False, name='Conv_1', activation='relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
        "    # acc: 73% (person) 77% (car)\n",
        "    # ---build block---\n",
        "    # x = layers.Conv2D(16, kernel_size=3, strides=2, use_bias=False, name='Conv1', activation='relu')(img_input)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "    # x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    # x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=5)\n",
        "\n",
        "    # x = layers.Conv2D(128, kernel_size=1, use_bias=False, name='Conv_1', activation='relu')(x)\n",
        "    # x = layers.GlobalAveragePooling2D()(x)\n",
        "    # x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
        "    # ---acc = 70/68%---\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    # Depthwise\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation='relu',\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    # x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation='relu',\n",
        "                      name=prefix + 'pointwise')(x)\n",
        "\n",
        "    # if in_channels == pointwise_filters and stride == 1:\n",
        "    #     return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2_2(epochs, batch_size, save_model_name, rotate):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((10000,96,96,1))\n",
        "    test_label = np.empty((10000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==999:break\n",
        "    # print('due to RAM restriction of Colab, we only load 10000 test images')\n",
        "    print('For bike + motorcycle data, we only use 1000 test images')\n",
        "    class_weight = {0:1., 1:(train_size-np.sum(train_label))/np.sum(train_label)}\n",
        "    print('class weight: ',class_weight)\n",
        "    if rotate:\n",
        "      train_data = np.transpose(train_data, axes=(0,2,1,3))\n",
        "      test_data = np.transpose(test_data, axes=(0,2,1,3))\n",
        "    print(train_data.shape)\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(train_data[i,:,:,0], cmap=\"gray\")\n",
        "        plt.xlabel(train_label[i])\n",
        "    plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "    print(np.sum(test_label))\n",
        "    if rotate:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    else:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_fliphorizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "    model.fit(train_data, train_label,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(test_data, test_label),\n",
        "              class_weight=class_weight)\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 25\n",
        "batch_size = 128\n",
        "main_mobileV2_2(epochs, batch_size, \"vww_mobile_v5_%d\"%epochs, rotate=True)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_dues\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_0_depthwise (Depthwise (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_0_pointwise (Conv2D)   (None, 47, 47, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_1_depthwise (Depthwise (None, 23, 23, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_1_pointwise (Conv2D)   (None, 23, 23, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_2_depthwise (Depthwise (None, 23, 23, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_2_pointwise (Conv2D)   (None, 23, 23, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_3_depthwise (Depthwise (None, 11, 11, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_3_pointwise (Conv2D)   (None, 11, 11, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_4_depthwise (Depthwise (None, 11, 11, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_4_pointwise (Conv2D)   (None, 11, 11, 64)        2048      \n",
            "_________________________________________________________________\n",
            "block_5_depthwise (Depthwise (None, 5, 5, 64)          576       \n",
            "_________________________________________________________________\n",
            "block_5_pointwise (Conv2D)   (None, 5, 5, 64)          4096      \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv2D)              (None, 5, 5, 128)         8192      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "Logits (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 20,994\n",
            "Trainable params: 20,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 83816 files belonging to 2 classes.\n",
            "Found 1045 files belonging to 2 classes.\n",
            "there are 83816 train images and 1045 test images.\n",
            "0/83816 train images loaded\n",
            "5000/83816 train images loaded\n",
            "10000/83816 train images loaded\n",
            "15000/83816 train images loaded\n",
            "20000/83816 train images loaded\n",
            "25000/83816 train images loaded\n",
            "30000/83816 train images loaded\n",
            "35000/83816 train images loaded\n",
            "40000/83816 train images loaded\n",
            "45000/83816 train images loaded\n",
            "50000/83816 train images loaded\n",
            "55000/83816 train images loaded\n",
            "60000/83816 train images loaded\n",
            "65000/83816 train images loaded\n",
            "70000/83816 train images loaded\n",
            "75000/83816 train images loaded\n",
            "80000/83816 train images loaded\n",
            "83816 train images loaded\n",
            "0/1045 test images loaded\n",
            "For bike + motorcycle data, we only use 1000 test images\n",
            "class weight:  {0: 1.0, 1: 10.70451054322022}\n",
            "(83816, 96, 96, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.6719560929919563e+264\n",
            "(83816, 96, 96, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "Epoch 1/25\n",
            "655/655 [==============================] - 72s 61ms/step - loss: 1.1130 - accuracy: 0.5989 - val_loss: nan - val_accuracy: 0.5332\n",
            "Epoch 2/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.9569 - accuracy: 0.7303 - val_loss: nan - val_accuracy: 0.5358\n",
            "Epoch 3/25\n",
            "655/655 [==============================] - 40s 60ms/step - loss: 0.8652 - accuracy: 0.7821 - val_loss: nan - val_accuracy: 0.5394\n",
            "Epoch 4/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.8363 - accuracy: 0.7987 - val_loss: nan - val_accuracy: 0.5397\n",
            "Epoch 5/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.8046 - accuracy: 0.8113 - val_loss: nan - val_accuracy: 0.5412\n",
            "Epoch 6/25\n",
            "655/655 [==============================] - 40s 60ms/step - loss: 0.7890 - accuracy: 0.8160 - val_loss: nan - val_accuracy: 0.5408\n",
            "Epoch 7/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.7770 - accuracy: 0.8214 - val_loss: nan - val_accuracy: 0.5401\n",
            "Epoch 8/25\n",
            "655/655 [==============================] - 40s 60ms/step - loss: 0.7499 - accuracy: 0.8292 - val_loss: nan - val_accuracy: 0.5416\n",
            "Epoch 9/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.7375 - accuracy: 0.8329 - val_loss: nan - val_accuracy: 0.5388\n",
            "Epoch 10/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.7272 - accuracy: 0.8367 - val_loss: nan - val_accuracy: 0.5420\n",
            "Epoch 11/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.7041 - accuracy: 0.8440 - val_loss: nan - val_accuracy: 0.5420\n",
            "Epoch 12/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.6878 - accuracy: 0.8455 - val_loss: nan - val_accuracy: 0.5410\n",
            "Epoch 13/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.6641 - accuracy: 0.8514 - val_loss: nan - val_accuracy: 0.5430\n",
            "Epoch 14/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.6548 - accuracy: 0.8545 - val_loss: nan - val_accuracy: 0.5423\n",
            "Epoch 15/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.6339 - accuracy: 0.8584 - val_loss: nan - val_accuracy: 0.5426\n",
            "Epoch 16/25\n",
            "655/655 [==============================] - 40s 60ms/step - loss: 0.6157 - accuracy: 0.8625 - val_loss: nan - val_accuracy: 0.5441\n",
            "Epoch 17/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.5986 - accuracy: 0.8661 - val_loss: nan - val_accuracy: 0.5440\n",
            "Epoch 18/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.5821 - accuracy: 0.8700 - val_loss: nan - val_accuracy: 0.5446\n",
            "Epoch 19/25\n",
            "655/655 [==============================] - 43s 65ms/step - loss: 0.5671 - accuracy: 0.8721 - val_loss: nan - val_accuracy: 0.5438\n",
            "Epoch 20/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.5586 - accuracy: 0.8748 - val_loss: nan - val_accuracy: 0.5422\n",
            "Epoch 21/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.5470 - accuracy: 0.8761 - val_loss: nan - val_accuracy: 0.5444\n",
            "Epoch 22/25\n",
            "655/655 [==============================] - 41s 62ms/step - loss: 0.5272 - accuracy: 0.8820 - val_loss: nan - val_accuracy: 0.5444\n",
            "Epoch 23/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.5258 - accuracy: 0.8787 - val_loss: nan - val_accuracy: 0.5431\n",
            "Epoch 24/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.4971 - accuracy: 0.8869 - val_loss: nan - val_accuracy: 0.5446\n",
            "Epoch 25/25\n",
            "655/655 [==============================] - 40s 61ms/step - loss: 0.4862 - accuracy: 0.8890 - val_loss: nan - val_accuracy: 0.5438\n",
            "79/79 [==============================] - 1s 10ms/step - loss: nan - accuracy: 0.5438\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: check_point/vww_mobile_v5_25/assets\n",
            "Before quantiz accuracy: 54.37999963760376%\n",
            "Quantized test accuracy: 80.19999694824219%\n",
            "loss acc : -25.81999969482422%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oB3EgMgA_BG"
      },
      "source": [
        "Simple CNN(後來有把一些CONV換成depthwise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ASX1pKC6TE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "outputId": "5dfa83cd-58ed-4a83-afb2-db2485f9461e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import cv2\n",
        "\n",
        "def main_cnn(epochs, batch_size, save_model_name, rotate):\n",
        "\n",
        "    # ----create model----\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=8, kernel_size=[3, 3], padding='valid',\n",
        "                                     activation=tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=[3, 3], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[1, 1], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[1, 1], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    # val accu: 71%\n",
        "    # ----create model----\n",
        "    # model = tf.keras.Sequential()\n",
        "    # model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=16, kernel_size=[3, 3], padding='valid',\n",
        "    #                                  activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.Flatten())\n",
        "    # model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    # model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # model.summary()\n",
        "    # val accu: 71%\n",
        "    # ----create model----\n",
        "    # model = tf.keras.Sequential()\n",
        "    # model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=16, kernel_size=[3, 3], padding='valid',\n",
        "    #                                  activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.Flatten())\n",
        "    # model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    # model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # model.summary()\n",
        "    # val accu: 66%    \n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((10000,96,96,1))\n",
        "    test_label = np.empty((10000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==9999:break\n",
        "    print('due to RAM restriction of Colab, we only load 10000 test images')\n",
        "    class_weight = {0:1., 1:(train_size-np.sum(train_label))/np.sum(train_label)}\n",
        "    print('class weight: ',class_weight)\n",
        "    if rotate:\n",
        "      train_data = np.transpose(train_data, axes=(0,2,1,3))\n",
        "      test_data = np.transpose(test_data, axes=(0,2,1,3))\n",
        "    print(train_data.shape)\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(train_data[i,:,:,0], cmap=\"gray\")\n",
        "        plt.xlabel(train_label[i])\n",
        "    plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "    print(np.sum(test_label))\n",
        "    if rotate:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    else:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_fliphorizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "    model.fit(train_data, train_label,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(test_data, test_label),\n",
        "              class_weight=class_weight)\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        # for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(1000):    \n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "        \n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "    print('model saved to ',save_model_name+\".tflite\")\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 15\n",
        "batch_size = 128\n",
        "main_cnn(epochs, batch_size, \"vww_cnn_v4_%d\"%epochs, rotate=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 94, 94, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 94, 94, 8)         584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 8)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_2 (Depthwis (None, 45, 45, 8)         72        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 45, 45, 16)        144       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1936)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                61984     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 65,250\n",
            "Trainable params: 65,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed5edeab656e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0mmain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vww_cnn_v4_%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ed5edeab656e>\u001b[0m in \u001b[0;36mmain_cnn\u001b[0;34m(epochs, batch_size, save_model_name, rotate)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         image_size=(96, 96))\n\u001b[0m\u001b[1;32m     73\u001b[0m     test_dataset = image_dataset_from_directory(\n\u001b[1;32m     74\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/dues_data/val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, smart_resize)\u001b[0m\n\u001b[1;32m    188\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       follow_links=follow_links)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msubdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dues_data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjp19S49BHjq"
      },
      "source": [
        "ResNet-like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4X0fIHOssRJ",
        "outputId": "0bb14219-a3a2-4cc2-d4e9-d240938086e1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "def Conv2D_BN(inputs,filter,kernel,padding,stride):\n",
        "    outputs = keras.layers.Conv2D(filters=filter,kernel_size=kernel,padding=padding,strides=stride,activation='relu')(inputs)\n",
        "    outputs = keras.layers.BatchNormalization()(outputs)\n",
        "    return outputs\n",
        "\n",
        "def residual_block(inputs,filter,stride,whether_identity_change=False):\n",
        "    x = Conv2D_BN(inputs, filter[0], kernel=(1,1), padding='same', stride=stride) \n",
        "    x = Conv2D_BN(x, filter[1], kernel=(3,3), padding='same', stride=1)\n",
        "    x = Conv2D_BN(x, filter[2] ,kernel=(1,1), padding='same', stride=1)\n",
        "\n",
        "    if whether_identity_change:\n",
        "        identity = Conv2D_BN(inputs, filter[2], kernel=(1,1), padding='same', stride=stride)\n",
        "        x = keras.layers.add([x,identity])\n",
        "        return x\n",
        "    else:\n",
        "        x = keras.layers.add([x,inputs])\n",
        "        return x\n",
        "def ResNet():\n",
        "    inputs = keras.Input(shape=(96,96,1))\n",
        "    x = Conv2D_BN(inputs,16,(3,3),'same',2)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=1, padding='same')(x)\n",
        "    x = residual_block(x,[16,16,16],1)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='same')(x)\n",
        "    x = residual_block(x,[16,16,16],1)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='same')(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(32,activation='relu')(x)\n",
        "    x = keras.layers.Dense(2,activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs,outputs=x)\n",
        "    #model.summary()\n",
        "    return model\n",
        "\n",
        "def main_res(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = ResNet()\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    # for i in range(3):\n",
        "    #     plt.subplot(1, 3, i + 1)\n",
        "    #     plt.imshow(train_data[i], cmap=\"gray\")\n",
        "    #     plt.xlabel(train_label[i])\n",
        "    # plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        # for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(1000):    \n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "        \n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "    print('model saved to ',save_model_name+\".tflite\")\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_res(epochs, batch_size, \"vww_res_%d\"%epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 96, 96, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 16)   160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 48, 48, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 48, 48, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 48, 48, 16)   272         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 48, 48, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 48, 48, 16)   2320        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 48, 48, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 48, 48, 16)   272         batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 48, 48, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 48, 48, 16)   0           batch_normalization_3[0][0]      \n",
            "                                                                 max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 24, 24, 16)   272         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 24, 24, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 24, 24, 16)   2320        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 24, 24, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 24, 24, 16)   272         batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 24, 24, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 24, 24, 16)   0           batch_normalization_6[0][0]      \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2304)         0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           73760       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            66          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 80,162\n",
            "Trainable params: 79,938\n",
            "Non-trainable params: 224\n",
            "__________________________________________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n",
            "75000/82783 train images loaded\n",
            "80000/82783 train images loaded\n",
            "82783 train images loaded\n",
            "0/40504 test images loaded\n",
            "due to RAM restriction of Colab, we only load 5000 test images\n",
            "(82783, 96, 96, 1)\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 50s 26ms/step - loss: 0.4857 - accuracy: 0.9250 - val_loss: 0.2239 - val_accuracy: 0.9334\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2249 - accuracy: 0.9322 - val_loss: 0.2189 - val_accuracy: 0.9334\n",
            "Epoch 3/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2194 - accuracy: 0.9320 - val_loss: 0.2184 - val_accuracy: 0.9334\n",
            "Epoch 4/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2194 - accuracy: 0.9305 - val_loss: 0.2173 - val_accuracy: 0.9334\n",
            "Epoch 5/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2120 - accuracy: 0.9324 - val_loss: 0.2208 - val_accuracy: 0.9334\n",
            "Epoch 6/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2080 - accuracy: 0.9339 - val_loss: 0.2169 - val_accuracy: 0.9334\n",
            "Epoch 7/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2004 - accuracy: 0.9344 - val_loss: 0.2158 - val_accuracy: 0.9334\n",
            "Epoch 8/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2012 - accuracy: 0.9333 - val_loss: 0.2330 - val_accuracy: 0.9334\n",
            "Epoch 9/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2030 - accuracy: 0.9321 - val_loss: 0.2327 - val_accuracy: 0.9334\n",
            "Epoch 10/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.1979 - accuracy: 0.9322 - val_loss: 0.2276 - val_accuracy: 0.9334\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2276 - accuracy: 0.9334\n",
            "INFO:tensorflow:Assets written to: check_point/vww_res_10/assets\n",
            "model saved to  vww_res_10.tflite\n",
            "Before quantiz accuracy: 93.33999752998352%\n",
            "Quantized test accuracy: 94.0%\n",
            "loss acc : -0.6600022315979004%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AknsENSIBMvy"
      },
      "source": [
        "MobileNet-like(架構沒調好train不動)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEVEXr7AssnP"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1.0,\n",
        "                classes=2):\n",
        "    # input_shape = (96,96,1)\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(img_input)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=10)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=11)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=12)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=13)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=14)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=15)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=16)\n",
        "\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "\n",
        "    x = layers.Conv2D(320,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax',\n",
        "                  use_bias=True, name='Logits')(x)\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=((2,2),(2,2)),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=0.35,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_mobileV2(epochs, batch_size, \"vww__mobile_full_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfdWtzaTBTd4"
      },
      "source": [
        "另一個MobileNet-like(架構沒調好train不動)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dRCXt0RbGJXf",
        "outputId": "7eec29bd-7b93-406d-d065-c82580f0eb80"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1.0,\n",
        "                classes=2):\n",
        "    # input_shape = (96,96,1)\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(img_input)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "\n",
        "    x = layers.Conv2D(128,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax',\n",
        "                  use_bias=True, name='Logits')(x)\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=((2,2),(2,2)),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=0.35,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_mobileV2(epochs, batch_size, \"vww__mobile_small_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_dues\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "Conv1_relu (ReLU)            (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "expanded_conv_depthwise (Dep (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "expanded_conv_depthwise_relu (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "expanded_conv_project (Conv2 (None, 47, 47, 8)         256       \n",
            "_________________________________________________________________\n",
            "block_1_expand (Conv2D)      (None, 47, 47, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_1_expand_relu (ReLU)   (None, 47, 47, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)  (None, 51, 51, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_depthwise (Depthwise (None, 25, 25, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_project (Conv2D)     (None, 25, 25, 8)         48        \n",
            "_________________________________________________________________\n",
            "block_2_expand (Conv2D)      (None, 25, 25, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_2_expand_relu (ReLU)   (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_2_depthwise (Depthwise (None, 25, 25, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_2_project (Conv2D)     (None, 25, 25, 8)         48        \n",
            "_________________________________________________________________\n",
            "block_3_expand (Conv2D)      (None, 25, 25, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_3_expand_relu (ReLU)   (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)  (None, 29, 29, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_4_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_4_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_4_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_4_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_5_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_5_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_5_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_5_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_6_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_6_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)  (None, 18, 18, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_6_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_6_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_7_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_7_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_7_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_7_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_8_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_8_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_8_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_8_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_9_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_9_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_9_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_9_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv2D)              (None, 8, 8, 128)         3072      \n",
            "_________________________________________________________________\n",
            "out_relu (ReLU)              (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "Logits (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 6,472\n",
            "Trainable params: 6,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-717b168b8839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m \u001b[0mmain_mobileV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vww__mobile_small_%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-717b168b8839>\u001b[0m in \u001b[0;36mmain_mobileV2\u001b[0;34m(epochs, batch_size, save_model_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2723\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2724\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}