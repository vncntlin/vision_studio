{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210520_VWW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTKNU1Iwyrts"
      },
      "source": [
        "# 下載資料\n",
        "\n",
        "我生好person, bicycle, car, traffic light的資料了，請至這個google drive連結下載然後傳到你自己的google drive\n",
        "https://drive.google.com/drive/folders/1ZOpWhs83X1dMMqB9NokK9sn2SDMorpqG?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qyiNj2tyriv"
      },
      "source": [
        "# 自己準備資料(可先跳過)\n",
        "\n",
        "如果你想自己弄別的class，或調整small object threshold，請把下面三個檔案換成這個zip檔裡面的檔案\n",
        "1. model/research/slim內的download_and_convert_data.py\n",
        "2. 和model/reseach/slim/datasets內的download_and_convert_visualwakewords.py和download_and_convert_visualwakewords_lib.py\n",
        "\n",
        "zip連結: https://drive.google.com/file/d/1acqxB0kmCENUxBABTjxSLjEETeNe9xe2/view?usp=sharing\n",
        "\n",
        "可以修改download_and_convert_data.py的small_object_area_threshold和foreground_class_of_interest\n",
        "\n",
        "當然也可以修改圖片resize的大小(不過model input也要改)\n",
        "\n",
        "然後在model/research/slim內執行\n",
        "python download_and_convert_data.py --dataset_name=visualwakewords --dataset_dir=/tmp/visualwakewords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311MPjKnBqRU"
      },
      "source": [
        "# 後續燒錄進板子(可先跳過)\n",
        "1. 下載tflite擋下來用xxd -i filename.tflite person_detect_model_data.cc把他轉成C++檔\n",
        "2. 包成C++檔之後，改掉檔案的頭尾。照抄原本專案裡的person_detect_model_data.cc頭尾就好了。\n",
        "3. 記得main_function.cc裡面的mutable_op_resolver下，要添加這個net需要的operations(OP必須照字母順序!)，model大時有可能需要改大kTensorArenaSize，不過太大的話也可能compile沒過。一些常見的error如下:\n",
        "\n",
        "  *   Arena size is too small for all buffers. Needed 176720 but only 137424 was available.\n",
        "AllocateTensors() failed\n",
        "  \n",
        "    -->改大kTensorArenaSize\n",
        "\n",
        "  *  Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'\n",
        "Failed to get registration from op code MAX_POOL_2D\n",
        "Failed starting model allocation.\n",
        "\n",
        "    -->改mutable_op_resolver的OP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9clHT8Wnhfl"
      },
      "source": [
        "# 訓練模型\n",
        "\n",
        "執行下一格之後照著指示做，可以讓colab存取google drive內的東西"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHh69wTBHwz8",
        "outputId": "a6fb774f-0006-4f59-b524-f574283d5cf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVZplqXQ3URV"
      },
      "source": [
        "!rm -rf dues_data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFMi7MgynQVM"
      },
      "source": [
        "下文的/content/drive/My Drive/20210521_vww_data/dues_data_person.zip請修改成自己的file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16dQ5dN9musu"
      },
      "source": [
        "!unzip '/content/drive/My Drive/20210521_vww_data/dues_data_car.zip' -d '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5imohVFtJ5Y"
      },
      "source": [
        "我寫了三種model: simple CNN, ResNet-like, MobileNetV2-like(full & small)\n",
        "\n",
        "2021/7/15 update: 新的mobileNet-like\n",
        "\n",
        "注意事項:\n",
        "1. model架構可以亂改，但注意model.summary()的時候output出來的total parameter數量最好不要超過500,000不然一定oversize。我暫時是試在100,000以內了啦\n",
        "2. ResNet(2 blocks)和MobileNetV2有點train不動，建議用CNN\n",
        "3. paper的準度是75%，有train到70%就還算不錯\n",
        "4. 因為資料量大，建議每次train之前kill一下OS，釋放出RAM(執行下方的block)\n",
        "5. 若每個class資料數量差異大，可以採用weighted classes。請參考: https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReiUUgQlwLOR"
      },
      "source": [
        "每次train之前，建議跑下面這個block以釋放RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmxurNX5vpwC"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN__ru2wLjo"
      },
      "source": [
        "\n",
        "------------------------------\n",
        "**下面幾個儲存格請選一個執行即可**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SgEEEsCqgyF",
        "outputId": "cbdcc8e2-974c-4da5-c2d4-ac637e8086c5"
      },
      "source": [
        "!dir\n",
        "!cp vww_v5_2_mobile__25.tflite '/content/drive/My Drive/20210521_vww_data/vww_v5_mobile_25_car.tflite'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check_point  sample_data\t\t vww_v5_mobile_10.tflite\n",
            "drive\t     vww_fcn_v4_15.tflite\t vww_v5_mobile_15.tflite\n",
            "dues_data    vww_v5_2_mobile__25.tflite  vww_v5_mobile_25.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J713fUUPTNMU",
        "outputId": "ad919fa3-c39e-4423-e490-6aa14ce9a090"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1), alpha=1.0, classes=2):\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # ---build block---\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=2, use_bias=False, name='Conv1', activation='relu')(img_input)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=5)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=1, use_bias=False, name='Conv_1', activation='relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
        "    # acc: 73% @ 25 iter\n",
        "    # ---build block---\n",
        "    # x = layers.Conv2D(16, kernel_size=3, strides=2, use_bias=False, name='Conv1', activation='relu')(img_input)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "    # x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    # x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=5)\n",
        "\n",
        "    # x = layers.Conv2D(128, kernel_size=1, use_bias=False, name='Conv_1', activation='relu')(x)\n",
        "    # x = layers.GlobalAveragePooling2D()(x)\n",
        "    # x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
        "    # ---acc = 70/68%---\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    # Depthwise\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation='relu',\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    # x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation='relu',\n",
        "                      name=prefix + 'pointwise')(x)\n",
        "\n",
        "    # if in_channels == pointwise_filters and stride == 1:\n",
        "    #     return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2_2(epochs, batch_size, save_model_name, rotate):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((10000,96,96,1))\n",
        "    test_label = np.empty((10000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "    class_weight = {0:1., 1:(train_size-np.sum(train_label))/np.sum(train_label)}\n",
        "    print('class weight: ',class_weight)\n",
        "    if rotate:\n",
        "      train_data = np.transpose(train_data, axes=(0,2,1,3))\n",
        "      test_data = np.transpose(test_data, axes=(0,2,1,3))\n",
        "    print(train_data.shape)\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(train_data[i,:,:,0], cmap=\"gray\")\n",
        "        plt.xlabel(train_label[i])\n",
        "    plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "    print(np.sum(test_label))\n",
        "    if rotate:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    else:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_fliphorizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "    model.fit(train_data, train_label,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(test_data, test_label),\n",
        "              class_weight=class_weight)\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 25\n",
        "batch_size = 128\n",
        "main_mobileV2_2(epochs, batch_size, \"vww_v5_mobile_%d\"%epochs, rotate=True)\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_dues\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_0_depthwise (Depthwise (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_0_pointwise (Conv2D)   (None, 47, 47, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_1_depthwise (Depthwise (None, 23, 23, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_1_pointwise (Conv2D)   (None, 23, 23, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_2_depthwise (Depthwise (None, 23, 23, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_2_pointwise (Conv2D)   (None, 23, 23, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_3_depthwise (Depthwise (None, 11, 11, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_3_pointwise (Conv2D)   (None, 11, 11, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_4_depthwise (Depthwise (None, 11, 11, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_4_pointwise (Conv2D)   (None, 11, 11, 64)        2048      \n",
            "_________________________________________________________________\n",
            "block_5_depthwise (Depthwise (None, 5, 5, 64)          576       \n",
            "_________________________________________________________________\n",
            "block_5_pointwise (Conv2D)   (None, 5, 5, 64)          4096      \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv2D)              (None, 5, 5, 128)         8192      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "Logits (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 20,994\n",
            "Trainable params: 20,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n",
            "75000/82783 train images loaded\n",
            "80000/82783 train images loaded\n",
            "82783 train images loaded\n",
            "0/40504 test images loaded\n",
            "5000/40504 test images loaded\n",
            "due to RAM restriction of Colab, we only load 10000 test images\n",
            "class weight:  {0: 1.0, 1: 13.706519808136436}\n",
            "(82783, 96, 96, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "651.0\n",
            "(82783, 96, 96, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "Epoch 1/25\n",
            "647/647 [==============================] - 73s 64ms/step - loss: 1.2658 - accuracy: 0.5088 - val_loss: 0.7874 - val_accuracy: 0.3323\n",
            "Epoch 2/25\n",
            "647/647 [==============================] - 40s 61ms/step - loss: 1.2099 - accuracy: 0.5538 - val_loss: 0.6636 - val_accuracy: 0.5887\n",
            "Epoch 3/25\n",
            "647/647 [==============================] - 40s 62ms/step - loss: 1.1675 - accuracy: 0.6175 - val_loss: 0.7837 - val_accuracy: 0.4519\n",
            "Epoch 4/25\n",
            "647/647 [==============================] - 40s 62ms/step - loss: 1.1376 - accuracy: 0.6512 - val_loss: 0.6430 - val_accuracy: 0.6376\n",
            "Epoch 5/25\n",
            "647/647 [==============================] - 40s 61ms/step - loss: 1.1099 - accuracy: 0.6674 - val_loss: 0.5696 - val_accuracy: 0.7081\n",
            "Epoch 6/25\n",
            "647/647 [==============================] - 40s 61ms/step - loss: 1.0989 - accuracy: 0.6806 - val_loss: 0.6677 - val_accuracy: 0.6203\n",
            "Epoch 7/25\n",
            "647/647 [==============================] - 40s 61ms/step - loss: 1.0742 - accuracy: 0.6930 - val_loss: 0.5367 - val_accuracy: 0.7348\n",
            "Epoch 8/25\n",
            "647/647 [==============================] - 40s 62ms/step - loss: 1.0679 - accuracy: 0.7008 - val_loss: 0.4500 - val_accuracy: 0.8215\n",
            "Epoch 9/25\n",
            "647/647 [==============================] - 40s 62ms/step - loss: 1.0540 - accuracy: 0.7060 - val_loss: 0.5503 - val_accuracy: 0.7263\n",
            "Epoch 10/25\n",
            "647/647 [==============================] - 39s 61ms/step - loss: 1.0417 - accuracy: 0.7126 - val_loss: 0.5887 - val_accuracy: 0.6947\n",
            "Epoch 11/25\n",
            "647/647 [==============================] - 40s 61ms/step - loss: 1.0213 - accuracy: 0.7210 - val_loss: 0.7456 - val_accuracy: 0.5606\n",
            "Epoch 12/25\n",
            "647/647 [==============================] - 39s 61ms/step - loss: 1.0172 - accuracy: 0.7204 - val_loss: 0.5125 - val_accuracy: 0.7601\n",
            "Epoch 13/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 1.0025 - accuracy: 0.7297 - val_loss: 0.4948 - val_accuracy: 0.7637\n",
            "Epoch 14/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 0.9887 - accuracy: 0.7344 - val_loss: 0.3512 - val_accuracy: 0.8647\n",
            "Epoch 15/25\n",
            "647/647 [==============================] - 38s 59ms/step - loss: 0.9789 - accuracy: 0.7368 - val_loss: 0.4879 - val_accuracy: 0.7733\n",
            "Epoch 16/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 0.9635 - accuracy: 0.7439 - val_loss: 0.4797 - val_accuracy: 0.7695\n",
            "Epoch 17/25\n",
            "647/647 [==============================] - 40s 62ms/step - loss: 0.9523 - accuracy: 0.7483 - val_loss: 0.5834 - val_accuracy: 0.7021\n",
            "Epoch 18/25\n",
            "647/647 [==============================] - 40s 62ms/step - loss: 0.9404 - accuracy: 0.7512 - val_loss: 0.5491 - val_accuracy: 0.7275\n",
            "Epoch 19/25\n",
            "647/647 [==============================] - 39s 61ms/step - loss: 0.9262 - accuracy: 0.7594 - val_loss: 0.4598 - val_accuracy: 0.7879\n",
            "Epoch 20/25\n",
            "647/647 [==============================] - 39s 61ms/step - loss: 0.9169 - accuracy: 0.7621 - val_loss: 0.6019 - val_accuracy: 0.6980\n",
            "Epoch 21/25\n",
            "647/647 [==============================] - 40s 61ms/step - loss: 0.9064 - accuracy: 0.7652 - val_loss: 0.6493 - val_accuracy: 0.6752\n",
            "Epoch 22/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 0.8937 - accuracy: 0.7685 - val_loss: 0.5913 - val_accuracy: 0.7108\n",
            "Epoch 23/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 0.8833 - accuracy: 0.7704 - val_loss: 0.5911 - val_accuracy: 0.7079\n",
            "Epoch 24/25\n",
            "647/647 [==============================] - 38s 59ms/step - loss: 0.8696 - accuracy: 0.7759 - val_loss: 0.4181 - val_accuracy: 0.8113\n",
            "Epoch 25/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 0.8693 - accuracy: 0.7733 - val_loss: 0.4274 - val_accuracy: 0.8104\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.4274 - accuracy: 0.8104\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: check_point/vww_v5_2_mobile__25/assets\n",
            "Before quantiz accuracy: 81.04000091552734%\n",
            "Quantized test accuracy: 79.69999694824219%\n",
            "loss acc : 1.3400018215179443%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ASX1pKC6TE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "outputId": "5dfa83cd-58ed-4a83-afb2-db2485f9461e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import cv2\n",
        "\n",
        "def main_cnn(epochs, batch_size, save_model_name, rotate):\n",
        "\n",
        "    # ----create model----\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=8, kernel_size=[3, 3], padding='valid',\n",
        "                                     activation=tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=[3, 3], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[1, 1], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[1, 1], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    # val accu: 71%\n",
        "    # ----create model----\n",
        "    # model = tf.keras.Sequential()\n",
        "    # model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=16, kernel_size=[3, 3], padding='valid',\n",
        "    #                                  activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.Flatten())\n",
        "    # model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    # model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # model.summary()\n",
        "    # val accu: 71%\n",
        "    # ----create model----\n",
        "    # model = tf.keras.Sequential()\n",
        "    # model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=16, kernel_size=[3, 3], padding='valid',\n",
        "    #                                  activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.Flatten())\n",
        "    # model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    # model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # model.summary()\n",
        "    # val accu: 66%    \n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((10000,96,96,1))\n",
        "    test_label = np.empty((10000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==9999:break\n",
        "    print('due to RAM restriction of Colab, we only load 10000 test images')\n",
        "    class_weight = {0:1., 1:(train_size-np.sum(train_label))/np.sum(train_label)}\n",
        "    print('class weight: ',class_weight)\n",
        "    if rotate:\n",
        "      train_data = np.transpose(train_data, axes=(0,2,1,3))\n",
        "      test_data = np.transpose(test_data, axes=(0,2,1,3))\n",
        "    print(train_data.shape)\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(train_data[i,:,:,0], cmap=\"gray\")\n",
        "        plt.xlabel(train_label[i])\n",
        "    plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "    print(np.sum(test_label))\n",
        "    if rotate:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    else:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_fliphorizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "    model.fit(train_data, train_label,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(test_data, test_label),\n",
        "              class_weight=class_weight)\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        # for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(1000):    \n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "        \n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "    print('model saved to ',save_model_name+\".tflite\")\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 15\n",
        "batch_size = 128\n",
        "main_cnn(epochs, batch_size, \"vww_cnn_v4_%d\"%epochs, rotate=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 94, 94, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 94, 94, 8)         584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 8)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_2 (Depthwis (None, 45, 45, 8)         72        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 45, 45, 16)        144       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1936)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                61984     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 65,250\n",
            "Trainable params: 65,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed5edeab656e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0mmain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vww_cnn_v4_%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ed5edeab656e>\u001b[0m in \u001b[0;36mmain_cnn\u001b[0;34m(epochs, batch_size, save_model_name, rotate)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         image_size=(96, 96))\n\u001b[0m\u001b[1;32m     73\u001b[0m     test_dataset = image_dataset_from_directory(\n\u001b[1;32m     74\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/dues_data/val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, smart_resize)\u001b[0m\n\u001b[1;32m    188\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       follow_links=follow_links)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msubdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dues_data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4X0fIHOssRJ",
        "outputId": "0bb14219-a3a2-4cc2-d4e9-d240938086e1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "def Conv2D_BN(inputs,filter,kernel,padding,stride):\n",
        "    outputs = keras.layers.Conv2D(filters=filter,kernel_size=kernel,padding=padding,strides=stride,activation='relu')(inputs)\n",
        "    outputs = keras.layers.BatchNormalization()(outputs)\n",
        "    return outputs\n",
        "\n",
        "def residual_block(inputs,filter,stride,whether_identity_change=False):\n",
        "    x = Conv2D_BN(inputs, filter[0], kernel=(1,1), padding='same', stride=stride) \n",
        "    x = Conv2D_BN(x, filter[1], kernel=(3,3), padding='same', stride=1)\n",
        "    x = Conv2D_BN(x, filter[2] ,kernel=(1,1), padding='same', stride=1)\n",
        "\n",
        "    if whether_identity_change:\n",
        "        identity = Conv2D_BN(inputs, filter[2], kernel=(1,1), padding='same', stride=stride)\n",
        "        x = keras.layers.add([x,identity])\n",
        "        return x\n",
        "    else:\n",
        "        x = keras.layers.add([x,inputs])\n",
        "        return x\n",
        "def ResNet():\n",
        "    inputs = keras.Input(shape=(96,96,1))\n",
        "    x = Conv2D_BN(inputs,16,(3,3),'same',2)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=1, padding='same')(x)\n",
        "    x = residual_block(x,[16,16,16],1)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='same')(x)\n",
        "    x = residual_block(x,[16,16,16],1)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='same')(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(32,activation='relu')(x)\n",
        "    x = keras.layers.Dense(2,activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs,outputs=x)\n",
        "    #model.summary()\n",
        "    return model\n",
        "\n",
        "def main_res(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = ResNet()\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    # for i in range(3):\n",
        "    #     plt.subplot(1, 3, i + 1)\n",
        "    #     plt.imshow(train_data[i], cmap=\"gray\")\n",
        "    #     plt.xlabel(train_label[i])\n",
        "    # plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        # for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(1000):    \n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "        \n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "    print('model saved to ',save_model_name+\".tflite\")\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_res(epochs, batch_size, \"vww_res_%d\"%epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 96, 96, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 16)   160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 48, 48, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 48, 48, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 48, 48, 16)   272         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 48, 48, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 48, 48, 16)   2320        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 48, 48, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 48, 48, 16)   272         batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 48, 48, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 48, 48, 16)   0           batch_normalization_3[0][0]      \n",
            "                                                                 max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 24, 24, 16)   272         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 24, 24, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 24, 24, 16)   2320        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 24, 24, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 24, 24, 16)   272         batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 24, 24, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 24, 24, 16)   0           batch_normalization_6[0][0]      \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2304)         0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           73760       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            66          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 80,162\n",
            "Trainable params: 79,938\n",
            "Non-trainable params: 224\n",
            "__________________________________________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n",
            "75000/82783 train images loaded\n",
            "80000/82783 train images loaded\n",
            "82783 train images loaded\n",
            "0/40504 test images loaded\n",
            "due to RAM restriction of Colab, we only load 5000 test images\n",
            "(82783, 96, 96, 1)\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 50s 26ms/step - loss: 0.4857 - accuracy: 0.9250 - val_loss: 0.2239 - val_accuracy: 0.9334\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2249 - accuracy: 0.9322 - val_loss: 0.2189 - val_accuracy: 0.9334\n",
            "Epoch 3/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2194 - accuracy: 0.9320 - val_loss: 0.2184 - val_accuracy: 0.9334\n",
            "Epoch 4/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2194 - accuracy: 0.9305 - val_loss: 0.2173 - val_accuracy: 0.9334\n",
            "Epoch 5/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2120 - accuracy: 0.9324 - val_loss: 0.2208 - val_accuracy: 0.9334\n",
            "Epoch 6/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2080 - accuracy: 0.9339 - val_loss: 0.2169 - val_accuracy: 0.9334\n",
            "Epoch 7/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2004 - accuracy: 0.9344 - val_loss: 0.2158 - val_accuracy: 0.9334\n",
            "Epoch 8/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2012 - accuracy: 0.9333 - val_loss: 0.2330 - val_accuracy: 0.9334\n",
            "Epoch 9/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2030 - accuracy: 0.9321 - val_loss: 0.2327 - val_accuracy: 0.9334\n",
            "Epoch 10/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.1979 - accuracy: 0.9322 - val_loss: 0.2276 - val_accuracy: 0.9334\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2276 - accuracy: 0.9334\n",
            "INFO:tensorflow:Assets written to: check_point/vww_res_10/assets\n",
            "model saved to  vww_res_10.tflite\n",
            "Before quantiz accuracy: 93.33999752998352%\n",
            "Quantized test accuracy: 94.0%\n",
            "loss acc : -0.6600022315979004%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEVEXr7AssnP"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1.0,\n",
        "                classes=2):\n",
        "    # input_shape = (96,96,1)\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(img_input)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=10)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=11)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=12)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=13)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=14)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=15)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=16)\n",
        "\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "\n",
        "    x = layers.Conv2D(320,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax',\n",
        "                  use_bias=True, name='Logits')(x)\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=((2,2),(2,2)),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=0.35,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_mobileV2(epochs, batch_size, \"vww__mobile_full_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dRCXt0RbGJXf",
        "outputId": "7eec29bd-7b93-406d-d065-c82580f0eb80"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1.0,\n",
        "                classes=2):\n",
        "    # input_shape = (96,96,1)\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(img_input)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "\n",
        "    x = layers.Conv2D(128,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax',\n",
        "                  use_bias=True, name='Logits')(x)\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=((2,2),(2,2)),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=0.35,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_mobileV2(epochs, batch_size, \"vww__mobile_small_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_dues\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "Conv1_relu (ReLU)            (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "expanded_conv_depthwise (Dep (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "expanded_conv_depthwise_relu (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "expanded_conv_project (Conv2 (None, 47, 47, 8)         256       \n",
            "_________________________________________________________________\n",
            "block_1_expand (Conv2D)      (None, 47, 47, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_1_expand_relu (ReLU)   (None, 47, 47, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)  (None, 51, 51, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_depthwise (Depthwise (None, 25, 25, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_project (Conv2D)     (None, 25, 25, 8)         48        \n",
            "_________________________________________________________________\n",
            "block_2_expand (Conv2D)      (None, 25, 25, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_2_expand_relu (ReLU)   (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_2_depthwise (Depthwise (None, 25, 25, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_2_project (Conv2D)     (None, 25, 25, 8)         48        \n",
            "_________________________________________________________________\n",
            "block_3_expand (Conv2D)      (None, 25, 25, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_3_expand_relu (ReLU)   (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)  (None, 29, 29, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_4_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_4_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_4_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_4_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_5_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_5_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_5_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_5_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_6_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_6_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)  (None, 18, 18, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_6_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_6_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_7_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_7_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_7_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_7_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_8_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_8_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_8_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_8_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_9_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_9_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_9_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_9_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv2D)              (None, 8, 8, 128)         3072      \n",
            "_________________________________________________________________\n",
            "out_relu (ReLU)              (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "Logits (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 6,472\n",
            "Trainable params: 6,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-717b168b8839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m \u001b[0mmain_mobileV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vww__mobile_small_%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-717b168b8839>\u001b[0m in \u001b[0;36mmain_mobileV2\u001b[0;34m(epochs, batch_size, save_model_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2723\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2724\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}