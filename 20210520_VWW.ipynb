{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210520_VWW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTKNU1Iwyrts"
      },
      "source": [
        "# 下載資料\n",
        "\n",
        "我生好person, bicycle, car, traffic light的資料了，請至這個google drive連結下載然後傳到你自己的google drive\n",
        "https://drive.google.com/drive/folders/1ZOpWhs83X1dMMqB9NokK9sn2SDMorpqG?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qyiNj2tyriv"
      },
      "source": [
        "# 自己準備資料(可先跳過)\n",
        "\n",
        "如果你想自己弄別的class，或調整small object threshold，請把下面三個檔案換成這個zip檔裡面的檔案\n",
        "1. model/research/slim內的download_and_convert_data.py\n",
        "2. 和model/reseach/slim/datasets內的download_and_convert_visualwakewords.py和download_and_convert_visualwakewords_lib.py\n",
        "\n",
        "zip連結: https://drive.google.com/file/d/1acqxB0kmCENUxBABTjxSLjEETeNe9xe2/view?usp=sharing\n",
        "\n",
        "可以修改download_and_convert_data.py的small_object_area_threshold和foreground_class_of_interest\n",
        "\n",
        "當然也可以修改圖片resize的大小(不過model input也要改)\n",
        "\n",
        "然後在model/research/slim內執行\n",
        "python download_and_convert_data.py --dataset_name=visualwakewords --dataset_dir=/tmp/visualwakewords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "311MPjKnBqRU"
      },
      "source": [
        "# 後續燒錄進板子(可先跳過)\n",
        "1. 下載tflite擋下來用xxd -i filename.tflite person_detect_model_data.cc把他轉成C++檔\n",
        "2. 包成C++檔之後，改掉檔案的頭尾。照抄原本專案裡的person_detect_model_data.cc頭尾就好了。\n",
        "3. 記得main_function.cc裡面的mutable_op_resolver下，要添加這個net需要的operations(OP必須照字母順序!)，model大時有可能需要改大kTensorArenaSize，不過太大的話也可能compile沒過。一些常見的error如下:\n",
        "\n",
        "  *   Arena size is too small for all buffers. Needed 176720 but only 137424 was available.\n",
        "AllocateTensors() failed\n",
        "  \n",
        "    -->改大kTensorArenaSize\n",
        "\n",
        "  *  Didn't find op for builtin opcode 'MAX_POOL_2D' version '2'\n",
        "Failed to get registration from op code MAX_POOL_2D\n",
        "Failed starting model allocation.\n",
        "\n",
        "    -->改mutable_op_resolver的OP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9clHT8Wnhfl"
      },
      "source": [
        "# 訓練模型\n",
        "\n",
        "執行下一格之後照著指示做，可以讓colab存取google drive內的東西"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHh69wTBHwz8",
        "outputId": "a6fb774f-0006-4f59-b524-f574283d5cf7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVZplqXQ3URV"
      },
      "source": [
        "!rm -rf dues_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFMi7MgynQVM"
      },
      "source": [
        "下文的/content/drive/My Drive/20210521_vww_data/dues_data_person.zip請修改成自己的file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16dQ5dN9musu"
      },
      "source": [
        "!unzip '/content/drive/My Drive/20210521_vww_data/dues_data_person.zip' -d '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5imohVFtJ5Y"
      },
      "source": [
        "我寫了三種model: simple CNN, ResNet-like, MobileNetV2-like(full & small)\n",
        "\n",
        "2021/7/15 update: 新的mobileNet-like\n",
        "\n",
        "注意事項:\n",
        "1. model架構可以亂改，但注意model.summary()的時候output出來的total parameter數量最好不要超過500,000不然一定oversize。我暫時是試在100,000以內了啦\n",
        "2. ResNet(2 blocks)和MobileNetV2有點train不動，建議用CNN\n",
        "3. paper的準度是75%，有train到70%就還算不錯\n",
        "4. 因為資料量大，建議每次train之前kill一下OS，釋放出RAM(執行下方的block)\n",
        "5. 若每個class資料數量差異大，可以採用weighted classes。請參考: https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReiUUgQlwLOR"
      },
      "source": [
        "每次train之前，建議跑下面這個block以釋放RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmxurNX5vpwC"
      },
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN__ru2wLjo"
      },
      "source": [
        "\n",
        "------------------------------\n",
        "**下面幾個儲存格請選一個執行即可**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J713fUUPTNMU",
        "outputId": "a0d289b9-b981-47c1-9008-3a274f18c61c"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1), alpha=1.0, classes=2):\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    # ---build block---\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=2, use_bias=False, name='Conv1', activation='relu')(img_input)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=5)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=1, use_bias=False, name='Conv_1', activation='relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
        "    # ---build block---\n",
        "    # x = layers.Conv2D(16, kernel_size=3, strides=2, use_bias=False, name='Conv1', activation='relu')(img_input)\n",
        "\n",
        "    # x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
        "    # x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
        "    # x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
        "    # x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=5)\n",
        "\n",
        "    # x = layers.Conv2D(128, kernel_size=1, use_bias=False, name='Conv_1', activation='relu')(x)\n",
        "    # x = layers.GlobalAveragePooling2D()(x)\n",
        "    # x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
        "    # ---acc = 70/68%---\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    # Depthwise\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation='relu',\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    # x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation='relu',\n",
        "                      name=prefix + 'pointwise')(x)\n",
        "\n",
        "    # if in_channels == pointwise_filters and stride == 1:\n",
        "    #     return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2_2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 25\n",
        "batch_size = 128\n",
        "main_mobileV2_2(epochs, batch_size, \"vww_v5_mobile_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_dues\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_0_depthwise (Depthwise (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_0_pointwise (Conv2D)   (None, 47, 47, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_1_depthwise (Depthwise (None, 23, 23, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_1_pointwise (Conv2D)   (None, 23, 23, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_2_depthwise (Depthwise (None, 23, 23, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_2_pointwise (Conv2D)   (None, 23, 23, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_3_depthwise (Depthwise (None, 11, 11, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_3_pointwise (Conv2D)   (None, 11, 11, 32)        1024      \n",
            "_________________________________________________________________\n",
            "block_4_depthwise (Depthwise (None, 11, 11, 32)        288       \n",
            "_________________________________________________________________\n",
            "block_4_pointwise (Conv2D)   (None, 11, 11, 64)        2048      \n",
            "_________________________________________________________________\n",
            "block_5_depthwise (Depthwise (None, 5, 5, 64)          576       \n",
            "_________________________________________________________________\n",
            "block_5_pointwise (Conv2D)   (None, 5, 5, 64)          4096      \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv2D)              (None, 5, 5, 128)         8192      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "Logits (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 20,994\n",
            "Trainable params: 20,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n",
            "75000/82783 train images loaded\n",
            "80000/82783 train images loaded\n",
            "82783 train images loaded\n",
            "0/40504 test images loaded\n",
            "due to RAM restriction of Colab, we only load 5000 test images\n",
            "(82783, 96, 96, 1)\n",
            "Epoch 1/25\n",
            "647/647 [==============================] - 72s 63ms/step - loss: 0.6858 - accuracy: 0.5456 - val_loss: 0.6779 - val_accuracy: 0.5880\n",
            "Epoch 2/25\n",
            "647/647 [==============================] - 39s 60ms/step - loss: 0.6574 - accuracy: 0.6090 - val_loss: 0.6392 - val_accuracy: 0.6438\n",
            "Epoch 3/25\n",
            "255/647 [==========>...................] - ETA: 23s - loss: 0.6276 - accuracy: 0.6512"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ASX1pKC6TE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "outputId": "5dfa83cd-58ed-4a83-afb2-db2485f9461e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import cv2\n",
        "\n",
        "def main_cnn(epochs, batch_size, save_model_name, rotate):\n",
        "\n",
        "    # ----create model----\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=8, kernel_size=[3, 3], padding='valid',\n",
        "                                     activation=tf.nn.relu))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=[3, 3], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[1, 1], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu', use_bias=False))\n",
        "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[1, 1], padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    model.summary()\n",
        "    # val accu: 71%\n",
        "    # ----create model----\n",
        "    # model = tf.keras.Sequential()\n",
        "    # model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=16, kernel_size=[3, 3], padding='valid',\n",
        "    #                                  activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.Flatten())\n",
        "    # model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    # model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # model.summary()\n",
        "    # val accu: 71%\n",
        "    # ----create model----\n",
        "    # model = tf.keras.Sequential()\n",
        "    # model.add(tf.keras.layers.Conv2D(input_shape=(96, 96, 1), filters=16, kernel_size=[3, 3], padding='valid',\n",
        "    #                                  activation=tf.nn.relu))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2))\n",
        "    # model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], padding='valid', activation='relu'))\n",
        "    # model.add(tf.keras.layers.Flatten())\n",
        "    # model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "    # model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "    # model.summary()\n",
        "    # val accu: 66%    \n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((10000,96,96,1))\n",
        "    test_label = np.empty((10000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==9999:break\n",
        "    print('due to RAM restriction of Colab, we only load 10000 test images')\n",
        "    class_weight = {0:1., 1:(train_size-np.sum(train_label))/np.sum(train_label)}\n",
        "    print('class weight: ',class_weight)\n",
        "    if rotate:\n",
        "      train_data = np.transpose(train_data, axes=(0,2,1,3))\n",
        "      test_data = np.transpose(test_data, axes=(0,2,1,3))\n",
        "    print(train_data.shape)\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.imshow(train_data[i,:,:,0], cmap=\"gray\")\n",
        "        plt.xlabel(train_label[i])\n",
        "    plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "    print(np.sum(test_label))\n",
        "    if rotate:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    else:\n",
        "      datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        vertical_fliphorizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "    model.fit(train_data, train_label,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(test_data, test_label),\n",
        "              class_weight=class_weight)\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        # for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(1000):    \n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "        \n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "    print('model saved to ',save_model_name+\".tflite\")\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 15\n",
        "batch_size = 128\n",
        "main_cnn(epochs, batch_size, \"vww_cnn_v4_%d\"%epochs, rotate=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 94, 94, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 94, 94, 8)         584       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 47, 47, 8)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_2 (Depthwis (None, 45, 45, 8)         72        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 45, 45, 16)        144       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1936)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                61984     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 65,250\n",
            "Trainable params: 65,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed5edeab656e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0mmain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vww_cnn_v4_%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-ed5edeab656e>\u001b[0m in \u001b[0;36mmain_cnn\u001b[0;34m(epochs, batch_size, save_model_name, rotate)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         image_size=(96, 96))\n\u001b[0m\u001b[1;32m     73\u001b[0m     test_dataset = image_dataset_from_directory(\n\u001b[1;32m     74\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/dues_data/val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, smart_resize)\u001b[0m\n\u001b[1;32m    188\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       follow_links=follow_links)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/preprocessing/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msubdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dues_data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4X0fIHOssRJ",
        "outputId": "0bb14219-a3a2-4cc2-d4e9-d240938086e1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "def Conv2D_BN(inputs,filter,kernel,padding,stride):\n",
        "    outputs = keras.layers.Conv2D(filters=filter,kernel_size=kernel,padding=padding,strides=stride,activation='relu')(inputs)\n",
        "    outputs = keras.layers.BatchNormalization()(outputs)\n",
        "    return outputs\n",
        "\n",
        "def residual_block(inputs,filter,stride,whether_identity_change=False):\n",
        "    x = Conv2D_BN(inputs, filter[0], kernel=(1,1), padding='same', stride=stride) \n",
        "    x = Conv2D_BN(x, filter[1], kernel=(3,3), padding='same', stride=1)\n",
        "    x = Conv2D_BN(x, filter[2] ,kernel=(1,1), padding='same', stride=1)\n",
        "\n",
        "    if whether_identity_change:\n",
        "        identity = Conv2D_BN(inputs, filter[2], kernel=(1,1), padding='same', stride=stride)\n",
        "        x = keras.layers.add([x,identity])\n",
        "        return x\n",
        "    else:\n",
        "        x = keras.layers.add([x,inputs])\n",
        "        return x\n",
        "def ResNet():\n",
        "    inputs = keras.Input(shape=(96,96,1))\n",
        "    x = Conv2D_BN(inputs,16,(3,3),'same',2)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(3,3), strides=1, padding='same')(x)\n",
        "    x = residual_block(x,[16,16,16],1)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='same')(x)\n",
        "    x = residual_block(x,[16,16,16],1)\n",
        "    x = keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding='same')(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(32,activation='relu')(x)\n",
        "    x = keras.layers.Dense(2,activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs,outputs=x)\n",
        "    #model.summary()\n",
        "    return model\n",
        "\n",
        "def main_res(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = ResNet()\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.average(train_data,axis=3)\n",
        "    #test_data = np.average(test_data,axis=3)\n",
        "    # for i in range(3):\n",
        "    #     plt.subplot(1, 3, i + 1)\n",
        "    #     plt.imshow(train_data[i], cmap=\"gray\")\n",
        "    #     plt.xlabel(train_label[i])\n",
        "    # plt.show()\n",
        "    \n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        # for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(1000):    \n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "        \n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "    print('model saved to ',save_model_name+\".tflite\")\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_res(epochs, batch_size, \"vww_res_%d\"%epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 96, 96, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 16)   160         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 48, 48, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 48, 48, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 48, 48, 16)   272         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 48, 48, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 48, 48, 16)   2320        batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 48, 48, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 48, 48, 16)   272         batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 48, 48, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 48, 48, 16)   0           batch_normalization_3[0][0]      \n",
            "                                                                 max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 16)   0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 24, 24, 16)   272         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 24, 24, 16)   64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 24, 24, 16)   2320        batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 24, 24, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 24, 24, 16)   272         batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 24, 24, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 24, 24, 16)   0           batch_normalization_6[0][0]      \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 16)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2304)         0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           73760       flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            66          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 80,162\n",
            "Trainable params: 79,938\n",
            "Non-trainable params: 224\n",
            "__________________________________________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n",
            "75000/82783 train images loaded\n",
            "80000/82783 train images loaded\n",
            "82783 train images loaded\n",
            "0/40504 test images loaded\n",
            "due to RAM restriction of Colab, we only load 5000 test images\n",
            "(82783, 96, 96, 1)\n",
            "Epoch 1/10\n",
            "647/647 [==============================] - 50s 26ms/step - loss: 0.4857 - accuracy: 0.9250 - val_loss: 0.2239 - val_accuracy: 0.9334\n",
            "Epoch 2/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2249 - accuracy: 0.9322 - val_loss: 0.2189 - val_accuracy: 0.9334\n",
            "Epoch 3/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2194 - accuracy: 0.9320 - val_loss: 0.2184 - val_accuracy: 0.9334\n",
            "Epoch 4/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2194 - accuracy: 0.9305 - val_loss: 0.2173 - val_accuracy: 0.9334\n",
            "Epoch 5/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2120 - accuracy: 0.9324 - val_loss: 0.2208 - val_accuracy: 0.9334\n",
            "Epoch 6/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2080 - accuracy: 0.9339 - val_loss: 0.2169 - val_accuracy: 0.9334\n",
            "Epoch 7/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2004 - accuracy: 0.9344 - val_loss: 0.2158 - val_accuracy: 0.9334\n",
            "Epoch 8/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2012 - accuracy: 0.9333 - val_loss: 0.2330 - val_accuracy: 0.9334\n",
            "Epoch 9/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.2030 - accuracy: 0.9321 - val_loss: 0.2327 - val_accuracy: 0.9334\n",
            "Epoch 10/10\n",
            "647/647 [==============================] - 16s 25ms/step - loss: 0.1979 - accuracy: 0.9322 - val_loss: 0.2276 - val_accuracy: 0.9334\n",
            "40/40 [==============================] - 0s 9ms/step - loss: 0.2276 - accuracy: 0.9334\n",
            "INFO:tensorflow:Assets written to: check_point/vww_res_10/assets\n",
            "model saved to  vww_res_10.tflite\n",
            "Before quantiz accuracy: 93.33999752998352%\n",
            "Quantized test accuracy: 94.0%\n",
            "loss acc : -0.6600022315979004%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEVEXr7AssnP"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1.0,\n",
        "                classes=2):\n",
        "    # input_shape = (96,96,1)\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(img_input)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=10)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=11)\n",
        "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=12)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=13)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=14)\n",
        "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=15)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=16)\n",
        "\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "\n",
        "    x = layers.Conv2D(320,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax',\n",
        "                  use_bias=True, name='Logits')(x)\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=((2,2),(2,2)),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=0.35,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_mobileV2(epochs, batch_size, \"vww__mobile_full_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dRCXt0RbGJXf",
        "outputId": "7eec29bd-7b93-406d-d065-c82580f0eb80"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "def MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=1.0,\n",
        "                classes=2):\n",
        "    # input_shape = (96,96,1)\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(32,\n",
        "                      kernel_size=3,\n",
        "                      strides=(2, 2),\n",
        "                      padding='valid',\n",
        "                      use_bias=False,\n",
        "                      name='Conv1')(img_input)\n",
        "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1,\n",
        "                            expansion=1, block_id=0)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=1)\n",
        "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=2)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=3)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=4)\n",
        "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=5)\n",
        "\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2,\n",
        "                            expansion=6, block_id=6)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=7)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=8)\n",
        "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1,\n",
        "                            expansion=6, block_id=9)\n",
        "\n",
        "\n",
        "\n",
        "    # no alpha applied to last conv as stated in the paper:\n",
        "    # if the width multiplier is greater than 1 we\n",
        "    # increase the number of output channels\n",
        "\n",
        "    x = layers.Conv2D(128,\n",
        "                      kernel_size=1,\n",
        "                      use_bias=False,\n",
        "                      name='Conv_1')(x)\n",
        "    x = layers.ReLU(6., name='out_relu')(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(classes, activation='softmax',\n",
        "                  use_bias=True, name='Logits')(x)\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = tf.keras.Model(inputs, x, name='mobilenetv2_dues')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
        "    channel_axis = -1\n",
        "\n",
        "    in_channels = 1\n",
        "    pointwise_conv_filters = int(filters * alpha)\n",
        "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
        "    x = inputs\n",
        "    prefix = 'block_{}_'.format(block_id)\n",
        "\n",
        "    if block_id:\n",
        "        # Expand\n",
        "        x = layers.Conv2D(expansion * in_channels,\n",
        "                          kernel_size=1,\n",
        "                          padding='same',\n",
        "                          use_bias=False,\n",
        "                          activation=None,\n",
        "                          name=prefix + 'expand')(x)\n",
        "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
        "    else:\n",
        "        prefix = 'expanded_conv_'\n",
        "\n",
        "    # Depthwise\n",
        "    if stride == 2:\n",
        "        x = layers.ZeroPadding2D(padding=((2,2),(2,2)),\n",
        "                                 name=prefix + 'pad')(x)\n",
        "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
        "                               strides=stride,\n",
        "                               activation=None,\n",
        "                               use_bias=False,\n",
        "                               padding='same' if stride == 1 else 'valid',\n",
        "                               name=prefix + 'depthwise')(x)\n",
        "\n",
        "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
        "\n",
        "    # Project\n",
        "    x = layers.Conv2D(pointwise_filters,\n",
        "                      kernel_size=1,\n",
        "                      padding='same',\n",
        "                      use_bias=False,\n",
        "                      activation=None,\n",
        "                      name=prefix + 'project')(x)\n",
        "\n",
        "    if in_channels == pointwise_filters and stride == 1:\n",
        "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
        "    return x\n",
        "def main_mobileV2(epochs, batch_size, save_model_name):\n",
        "    # ----create model----\n",
        "    model = MobileNetV2_Dues(input_shape=(96,96,1),\n",
        "                alpha=0.35,\n",
        "                classes=2)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # ----prepare dataset----\n",
        "\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/train',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        directory='/content/dues_data/val',\n",
        "        labels='inferred',\n",
        "        label_mode='binary',\n",
        "        color_mode='grayscale',\n",
        "        batch_size=1,\n",
        "        image_size=(96, 96))\n",
        "    train_size = len(train_dataset)\n",
        "    test_size = len(test_dataset)\n",
        "    print('there are %d train images and %d test images.'%(train_size,test_size))\n",
        "    train_data = np.empty((train_size,96,96,1))\n",
        "    train_label = np.empty((train_size,))\n",
        "    test_data = np.empty((5000,96,96,1))\n",
        "    test_label = np.empty((5000,))\n",
        "    for i, (image,label) in enumerate(train_dataset):\n",
        "        train_data[i] = image\n",
        "        train_label[i] = label\n",
        "        if i%5000==0: print('%d/%d train images loaded'%(i,train_size))\n",
        "        # if i==4999:break\n",
        "    print('%d train images loaded'%train_size)\n",
        "    for i, (image,label) in enumerate(test_dataset):\n",
        "        test_data[i] = image\n",
        "        test_label[i] = label\n",
        "        if i%5000==0: print('%d/%d test images loaded'%(i,test_size))\n",
        "        if i==4999:break\n",
        "    print('due to RAM restriction of Colab, we only load 5000 test images')\n",
        "\n",
        "    #train_data = np.expand_dims(train_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    #cifar (50000, 32, 32, 3, 1)\n",
        "    #mnist(60000, 28, 28, 1)\n",
        "    #test_data = np.expand_dims(test_data.astype(np.float32) - 128.0, axis=-1)\n",
        "    train_data = train_data.astype(np.float32) - 128.0\n",
        "    test_data = test_data.astype(np.float32) - 128.0\n",
        "\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        height_shift_range=0.1)\n",
        "    datagen.fit(train_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    \n",
        "    # ----train model----\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(train_data, train_label, epochs=epochs, batch_size=batch_size, validation_data=(test_data, test_label))\n",
        "\n",
        "    results = model.evaluate(test_data, test_label, batch_size=batch_size)\n",
        "\n",
        "    tf.saved_model.save(model, os.path.join(\"check_point\", save_model_name))\n",
        "\n",
        "    # ----quantize model----\n",
        "\n",
        "    def representative_data_gen():\n",
        "        for input_value in tf.data.Dataset.from_tensor_slices(test_data).batch(1).take(5000):\n",
        "            # Model has only one input so each data point has one element.\n",
        "            yield [input_value]\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(\"check_point\", save_model_name))\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model_quant = converter.convert()\n",
        "\n",
        "    with open(os.path.join(os.getcwd(), save_model_name+\".tflite\"), mode='wb') as f:\n",
        "        f.write(tflite_model_quant)\n",
        "\n",
        "    # ----test with quantize model----\n",
        "\n",
        "    interpreter = tf.lite.Interpreter(model_path=save_model_name+\".tflite\")\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    scales = output_details[0][\"quantization_parameters\"][\"scales\"][0]\n",
        "    zero_points = output_details[0][\"quantization_parameters\"][\"zero_points\"][0]\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    test_dataloader = tf.data.Dataset.from_tensor_slices((test_data[:1000], test_label[:1000]))\n",
        "    test_dataloader = test_dataloader.batch(batch_size=1)\n",
        "\n",
        "    quantized_test_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    for test_data, test_label in test_dataloader:\n",
        "        interpreter.set_tensor(input_details[0][\"index\"], tf.cast(test_data, dtype=tf.int8))\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        output = (output + (-zero_points)) * scales\n",
        "\n",
        "        predictions = tf.reshape(output, shape=(output.shape[0], 2))\n",
        "        quantized_test_acc.update_state(y_true=test_label, y_pred=predictions)\n",
        "\n",
        "    print(\"Before quantiz accuracy: {}%\".format(results[1] * 100))\n",
        "    print(\"Quantized test accuracy: {}%\".format(quantized_test_acc.result() * 100))\n",
        "    print(\"loss acc : {}%\".format((results[1] - quantized_test_acc.result()) * 100))\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "main_mobileV2(epochs, batch_size, \"vww__mobile_small_%d\"%epochs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mobilenetv2_dues\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
            "_________________________________________________________________\n",
            "Conv1 (Conv2D)               (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "Conv1_relu (ReLU)            (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "expanded_conv_depthwise (Dep (None, 47, 47, 32)        288       \n",
            "_________________________________________________________________\n",
            "expanded_conv_depthwise_relu (None, 47, 47, 32)        0         \n",
            "_________________________________________________________________\n",
            "expanded_conv_project (Conv2 (None, 47, 47, 8)         256       \n",
            "_________________________________________________________________\n",
            "block_1_expand (Conv2D)      (None, 47, 47, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_1_expand_relu (ReLU)   (None, 47, 47, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_pad (ZeroPadding2D)  (None, 51, 51, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_depthwise (Depthwise (None, 25, 25, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_1_depthwise_relu (ReLU (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_1_project (Conv2D)     (None, 25, 25, 8)         48        \n",
            "_________________________________________________________________\n",
            "block_2_expand (Conv2D)      (None, 25, 25, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_2_expand_relu (ReLU)   (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_2_depthwise (Depthwise (None, 25, 25, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_2_depthwise_relu (ReLU (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_2_project (Conv2D)     (None, 25, 25, 8)         48        \n",
            "_________________________________________________________________\n",
            "block_3_expand (Conv2D)      (None, 25, 25, 6)         48        \n",
            "_________________________________________________________________\n",
            "block_3_expand_relu (ReLU)   (None, 25, 25, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_pad (ZeroPadding2D)  (None, 29, 29, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_3_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_3_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_4_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_4_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_4_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_4_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_4_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_5_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_5_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_5_depthwise (Depthwise (None, 14, 14, 6)         54        \n",
            "_________________________________________________________________\n",
            "block_5_depthwise_relu (ReLU (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_5_project (Conv2D)     (None, 14, 14, 16)        96        \n",
            "_________________________________________________________________\n",
            "block_6_expand (Conv2D)      (None, 14, 14, 6)         96        \n",
            "_________________________________________________________________\n",
            "block_6_expand_relu (ReLU)   (None, 14, 14, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_6_pad (ZeroPadding2D)  (None, 18, 18, 6)         0         \n",
            "_________________________________________________________________\n",
            "block_6_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_6_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_6_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_7_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_7_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_7_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_7_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_7_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_8_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_8_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_8_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_8_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_8_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "block_9_expand (Conv2D)      (None, 8, 8, 6)           144       \n",
            "_________________________________________________________________\n",
            "block_9_expand_relu (ReLU)   (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_9_depthwise (Depthwise (None, 8, 8, 6)           54        \n",
            "_________________________________________________________________\n",
            "block_9_depthwise_relu (ReLU (None, 8, 8, 6)           0         \n",
            "_________________________________________________________________\n",
            "block_9_project (Conv2D)     (None, 8, 8, 24)          144       \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv2D)              (None, 8, 8, 128)         3072      \n",
            "_________________________________________________________________\n",
            "out_relu (ReLU)              (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "Logits (Dense)               (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 6,472\n",
            "Trainable params: 6,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 82783 files belonging to 2 classes.\n",
            "Found 40504 files belonging to 2 classes.\n",
            "there are 82783 train images and 40504 test images.\n",
            "0/82783 train images loaded\n",
            "5000/82783 train images loaded\n",
            "10000/82783 train images loaded\n",
            "15000/82783 train images loaded\n",
            "20000/82783 train images loaded\n",
            "25000/82783 train images loaded\n",
            "30000/82783 train images loaded\n",
            "35000/82783 train images loaded\n",
            "40000/82783 train images loaded\n",
            "45000/82783 train images loaded\n",
            "50000/82783 train images loaded\n",
            "55000/82783 train images loaded\n",
            "60000/82783 train images loaded\n",
            "65000/82783 train images loaded\n",
            "70000/82783 train images loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-717b168b8839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m \u001b[0mmain_mobileV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vww__mobile_small_%d\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-717b168b8839>\u001b[0m in \u001b[0;36mmain_mobileV2\u001b[0;34m(epochs, batch_size, save_model_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2723\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2724\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}